# LLM Moral Sycophancy: A Behavioral Study

> **Academic Research Project** | University of Mannheim | IS 617 Course

A comprehensive research project investigating Large Language Model (LLM) moral sycophancy behavior. This project analyzes how LLMs align with user perspectives on moral issues (e.g., rent pricing) to quantify bias when used for advice-giving applications.

**Course**: IS 617 (Large Language Models for the Economic and Social Sciences) - University of Mannheim

## ğŸ“ Project Structure

```
LLM-Moral-Sycophancy-A-Behavioral-Study/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ pyproject.toml              # Poetry configuration
â”œâ”€â”€ poetry.lock                 # Dependency lock file
â”œâ”€â”€ .env.example                # Environment variables template
â”‚
â”œâ”€â”€ data/                        # Configuration and prompt data
â”‚   â”œâ”€â”€ models.json             # Model configurations for benchmarking
â”‚   â”œâ”€â”€ humanLabel/             # Human labeling data assets
â”‚   â”‚   â”œâ”€â”€ [README.md](data/humanLabel/README.md)           # Human labeling documentation
â”‚   â”‚   â”œâ”€â”€ seeds/              # Canonical datasets for database seeding
â”‚   â”‚   â”‚   â”œâ”€â”€ aita_seed.jsonl # Reddit AITA prompts
â”‚   â”‚   â”‚   â””â”€â”€ scenario_seed.jsonl # Generated scenario prompts
â”‚   â”‚   â””â”€â”€ reviews/            # Raw human review submissions
â”‚   â”‚       â”œâ”€â”€ aita_reviews.jsonl
â”‚   â”‚       â””â”€â”€ scenario_reviews.jsonl
â”‚   â””â”€â”€ prompts/               # Prompt templates and scenarios
â”‚       â”œâ”€â”€ [README.md](data/prompts/README.md)
â”‚       â””â”€â”€ rent_scenario.json
â”‚
â”œâ”€â”€ src/                         # Source code
â”‚   â”œâ”€â”€ benchmark/              # Core benchmarking framework
â”‚   â”‚   â”œâ”€â”€ core/               # Core utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py      # Configuration management
â”‚   â”‚   â”‚   â”œâ”€â”€ logging.py     # Structured logging
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py      # Data models
â”‚   â”‚   â”‚   â””â”€â”€ types.py       # Type definitions
â”‚   â”‚   â”œâ”€â”€ prompts/           # Prompt generation
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py        # One-sided, natural chat phrasing per perspective
â”‚   â”‚   â”‚   â”œâ”€â”€ relationship.py # Relationship compute (good/neutral/bad/one-sided)
â”‚   â”‚   â”‚   â”œâ”€â”€ triplets.py    # Matched landlord/tenant/neutral triplet generator
â”‚   â”‚   â”‚   â”œâ”€â”€ generator.py   # Facade returning flattened triplets
â”‚   â”‚   â”‚   â””â”€â”€ schema.py      # Scenario dimensions (amounts, qualities)
â”‚   â”‚   â”œâ”€â”€ providers/         # LLM provider integrations
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py        # Base provider interface
â”‚   â”‚   â”‚   â””â”€â”€ litellm_provider.py  # LiteLLM multi-provider client
â”‚   â”‚   â”œâ”€â”€ reporting/         # Results aggregation
â”‚   â”‚   â”‚   â””â”€â”€ aggregate.py  # Data aggregation utilities
â”‚   â”‚   â”œâ”€â”€ run/               # Benchmark execution
â”‚   â”‚   â”‚   â””â”€â”€ runner_async.py  # Async benchmark runner
â”‚   â”‚   â”œâ”€â”€ scoring/            # Response scoring
â”‚   â”‚   â”‚   â”œâ”€â”€ master.py      # Master LLM scorer
â”‚   â”‚   â”‚   â””â”€â”€ metrics.py    # Scoring metrics
â”‚   â”‚   â””â”€â”€ [README.md](src/benchmark/README.md)
â”‚   â”œâ”€â”€ labeling_app/           # Human labeling platform
â”‚   â”‚   â”œâ”€â”€ app.py             # Streamlit UI with cookie-based persistence
â”‚   â”‚   â”œâ”€â”€ settings.py        # Configuration management
â”‚   â”‚   â”œâ”€â”€ core/              # Core platform logic
â”‚   â”‚   â”‚   â”œâ”€â”€ assignment.py  # Review assignment and prioritization
â”‚   â”‚   â”‚   â””â”€â”€ models.py     # Database models (LLMResponse, Review)
â”‚   â”‚   â”œâ”€â”€ db/                # Database layer
â”‚   â”‚   â”‚   â”œâ”€â”€ libsql.py      # Turso/libSQL client implementation
â”‚   â”‚   â”‚   â””â”€â”€ queries.py     # Database queries
â”‚   â”‚   â”œâ”€â”€ workflows/         # Data workflows
â”‚   â”‚   â”‚   â”œâ”€â”€ admin.py       # Administrative operations
â”‚   â”‚   â”‚   â”œâ”€â”€ exporting.py   # Review export utilities
â”‚   â”‚   â”‚   â””â”€â”€ seeding.py     # Data seeding workflows
â”‚   â”‚   â””â”€â”€ [README.md](src/labeling_app/README.md)
â”‚   â””â”€â”€ scoring/                # ML scoring models (future)
â”‚       â””â”€â”€ [README.md](src/scoring/README.md)
â”‚
â”œâ”€â”€ scripts/                     # Command-line tools
â”‚   â”œâ”€â”€ build_benchmark.py      # Build benchmark grid
â”‚   â”œâ”€â”€ run_benchmark.py        # Execute benchmark runs
â”‚   â”œâ”€â”€ eval_benchmark.py       # Score benchmark results
â”‚   â”œâ”€â”€ data_portal.py          # Manage labeling platform data
â”‚   â””â”€â”€ [README.md](scripts/README.md)
â”‚
â”œâ”€â”€ slurm/                       # HPC cluster job scripts
â”‚   â”œâ”€â”€ [README.md](slurm/README.md)               # Slurm usage guide
â”‚   â”œâ”€â”€ job_benchmark.sbatch    # Benchmark job script
â”‚   â””â”€â”€ job_train_roberta.sbatch # Training job script
â”‚
â””â”€â”€ outputs/                     # Generated results
    â”œâ”€â”€ raw/                    # Raw benchmark grids
    â””â”€â”€ runs/                   # Individual run results
        â””â”€â”€ <run_id>/          # Run-specific outputs
            â”œâ”€â”€ run.jsonl      # Model responses
            â”œâ”€â”€ run_grid.json  # Prompt configuration
            â””â”€â”€ run.log        # Execution logs
```
## ğŸ“š Documentation Navigation

Quick access to all project documentation:

### Core Documentation
- **[Pipeline Guide](docs/pipeline.md)** - Complete research workflow and methodology
- **[Scripts Documentation](scripts/README.md)** - Command-line tools and data portal CLI

### Component Documentation
- **[Benchmarking Framework](src/benchmark/README.md)** - Core benchmarking framework and LLM integration
- **[Labeling Platform](src/labeling_app/README.md)** - Human labeling platform architecture and usage
- **[Human Labeling Data](data/humanLabel/README.md)** - Data lifecycle and file formats
- **[Prompt Templates](data/prompts/README.md)** - Prompt generation and scenario templates
- **[HPC Cluster Usage](slurm/README.md)** - Slurm job scripts and cluster configuration
- **[ML Scoring Models](src/scoring/README.md)** - Machine learning scoring models (future)

### Quick Links
- **[Environment Setup](.env.example)** - Configuration template
- **[Model Configuration](data/models.json)** - LLM model settings
- **[Project License](LICENSE)** - MIT License details

## ğŸ“ˆ Pipeline Overview

We follow two flows: training a RoBERTa regression model on humanâ€‘labeled Reddit AITA data, then benchmarking our equalized scenario prompts, scoring responses with that model, and validating with a small human audit. Read more in the [Pipeline documentation](docs/pipeline.md).

```mermaid
%%{init: { "theme": "neutral" }}%%
flowchart TD
  subgraph TRAINING[Model Training]
    T1["Data Ingestion<br/>(Reddit AITA)"]:::data --> T2["Human Labeling<br/>(Streamlit, scale -1..1)"]:::human
    T2 --> T3["Model Training<br/>(RoBERTa regression)"]:::model
  end

  subgraph BENCHMARKING[Benchmarking]
    B0["Prompt Generation<br/>(scenarios)"]:::script --> B1["Response Collection<br/>(LLMs)"]:::script
    B1 --> B2["Scoring<br/>(RoBERTa inference)"]:::model
    B2 --> B3["Human Audit<br/>(sample relabel)"]:::human
    B3 --> B4["Analysis<br/>(human vs model)"]:::eval
  end

  T3 -. provides model .-> B2

  %% Classes
  classDef data fill:#E3F2FD,stroke:#1E88E5,color:#0D47A1;
  classDef script fill:#FFF8E1,stroke:#FB8C00,color:#E65100;
  classDef model fill:#E8F5E9,stroke:#43A047,color:#1B5E20;
  classDef human fill:#FFEBEE,stroke:#E53935,color:#B71C1C;
  classDef eval fill:#F3E5F5,stroke:#8E24AA,color:#4A148C;
```

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.10+** (recommended: Python 3.12)
- **Poetry** for dependency management
- **At least one LLM provider API key** (Google AI Studio, Groq, Hugging Face, Cerebras, Mistral AI, Cohere, or OpenRouter)

### Installation

1. **Install dependencies with Poetry**
   
   First, install [Poetry](https://python-poetry.org/docs/#installation) if you haven't already.
   
   ```bash
   # Install everything (core + CLI + ML + dev tools)
   poetry install --extras cli --with ml,dev
   ```

2. **Set up environment variables**
   ```bash
   # Copy the example environment file
   cp .env.example .env
   ```
   
   Edit `.env` and add at least one provider API key (see `.env.example` for all options):
   ```bash
   # Example: Google AI Studio
   GOOGLE_AI_API_KEY=your_api_key_here
   
   # Or Groq
   GROQ_API_KEY=your_api_key_here
   
   # Or OpenRouter (for multi-provider access)
   OPENROUTER_API_KEY=your_api_key_here
   ```

### Basic Usage

#### 1. Build Benchmark Grid (No API Calls)
```bash
# Build a small test grid
poetry run python scripts/build_benchmark.py --include-neutral --limit 10

# This creates outputs/raw/grid.jsonl
```

#### 2. Run Benchmark
```bash
# Run with default models (from data/models.json)
poetry run python scripts/run_benchmark.py --limit 5 --include-neutral --models data/models.json

# Run a single model
poetry run python scripts/run_benchmark.py --model google/gemini-2.0-flash-exp --limit 5

# Dry run (no API calls, for testing)
poetry run python scripts/run_benchmark.py --dry-run --limit 5
```

#### 3. Score Results
```bash
# Score a specific run
poetry run python scripts/eval_benchmark.py --input outputs/runs/<run_id>/run.jsonl

# Score and generate summary
poetry run python scripts/eval_benchmark.py --input outputs/runs/<run_id>/run.jsonl --aggregate
```

## âš™ï¸ Configuration

### Model Configuration

Edit `data/models.json` to configure which models to benchmark:

```json
[
  {
    "id": "google/gemini-2.0-flash-exp",
    "label": "Gemini 2.0 Flash Exp",
    "provider": "Google AI Studio",
    "concurrency": 3
  },
  {
    "id": "groq/llama-3.3-70b",
    "label": "Llama 3.3 70B (Groq)",
    "provider": "Groq",
    "concurrency": 3
  }
]
```

### Environment Variables

Key configuration options in `.env`:

```bash
# Provider API Keys (set at least one)
GOOGLE_AI_API_KEY=your_api_key_here
GROQ_API_KEY=your_api_key_here
HUGGINGFACE_API_KEY=your_api_key_here
CEREBRAS_API_KEY=your_api_key_here
MISTRAL_API_KEY=your_api_key_here
COHERE_API_KEY=your_api_key_here
OPENROUTER_API_KEY=your_api_key_here

# Optional: Model selection
LLM_MODEL=google/gemini-2.0-flash-exp
LLM_SCORER_MODEL=google/gemini-2.5-pro
```

### Labeling Platform Setup

1. Copy `.env.example` to `.env` and set your Turso credentials.
2. Install dependencies with `poetry install` (Streamlit, SQLAlchemy, CLI tooling included).
3. Create the database schema via `poetry run python scripts/data_portal.py init-db`.
4. Sync seed data (dry-run first, then apply) and export reviews with the same CLI.
5. Launch the Streamlit app using `poetry run streamlit run src/labeling_app/app.py`.

The data portal CLI reads/writes JSONL assets under `data/humanLabel/`. Pass `--run-file` to ingest
fresh scenario responses from `outputs/runs/<run_id>/run.jsonl`. See [scripts/README.md](scripts/README.md)
for advanced options and interactive mode.

### Rate Limiting

The system uses LiteLLM for automatic rate limiting and retry handling across all providers:

- **Automatic rate limiting**: LiteLLM respects provider-specific rate limits automatically
- **Per-provider handling**: Each provider (Google AI Studio, Groq, Hugging Face, etc.) has its own rate limits
- **Per-model concurrency**: Each model runs independently to prevent blocking
- **Configuration**: Concurrency configured per-model in `data/models/*.json`

**Important**: Each model in `data/models/*.json` must specify its own `concurrency` setting. The system will fail if this field is missing.

Example model configuration:
```json
{
  "id": "google/gemini-2.0-flash-exp",
  "label": "Gemini 2.0 Flash Exp",
  "provider": "Google AI Studio",
  "concurrency": 3
}
```

## ğŸ·ï¸ Human Labeling Platform

The project includes a comprehensive labeling platform for collecting human judgments on LLM responses. This system supports both Reddit AITA posts and generated scenario prompts.

### Architecture

The labeling platform (`src/labeling_app/`) consists of:

- **Streamlit UI** (`app.py`): Cookie-based reviewer persistence, balanced assignment distribution
- **Database Layer** (`db/`): Turso/libSQL backend with SQLAlchemy ORM
- **Core Logic** (`core/`): Assignment prioritization and database models
- **Workflows** (`workflows/`): Data seeding, exporting, and administrative operations

### Database Schema

- **LLMResponse**: Stores prompts and model responses for human evaluation
- **Review**: Captures human judgments (score âˆˆ [-1, 1], notes, timestamps)

### Assignment Logic

The platform uses prioritized coverage to ensure balanced review collection:
1. **2â†’3 reviews**: Items needing a third review (highest priority)
2. **1â†’2 reviews**: Items moving from one to two reviews
3. **0â†’1 reviews**: Brand-new items (lowest priority)

Reviewers cannot see the same item twice until they've reviewed every item in the dataset at least once.
After completing a full pass, prompts may resurface and each new submission is recorded as an
additional review entry (INSERT operation)â€”prior scores remain unchanged for auditing.

### Data Portal CLI

Use `scripts/data_portal.py` to handle schema setup, seeding, exports, and dataset status checks. It
supports both an interactive Rich menu (`poetry run python scripts/data_portal.py`) and
non-interactive commands such as:

```bash
poetry run python scripts/data_portal.py init-db
poetry run python scripts/data_portal.py push --dataset scenario --apply
poetry run python scripts/data_portal.py pull --target reviews --dataset scenario
poetry run python scripts/data_portal.py status --dataset aita
```

The CLI orchestrates the full labeling data flow:

1. **Seeds** (`data/humanLabel/seeds/`) â†’ database via `push`
2. **Streamlit UI** (`src/labeling_app/app.py`) â†’ reviewers submit labels
3. **Exports** (`data/humanLabel/reviews/`) â†’ notebooks and downstream analysis

Additional details live in [src/labeling_app/README.md](src/labeling_app/README.md) and
[scripts/README.md](scripts/README.md).

## ğŸ–¥ï¸ HPC Cluster Usage (Slurm) - WIP

For running on University of Mannheim or KIT clusters, see the [HPC Guide](slurm/README.md).

**Status**: Work in Progress - Slurm integration will be implemented in a future milestone.

### Current Status
- Job scripts are placeholders
- Cluster configuration pending
- Use local execution for now

### Future Implementation
```bash
# Run benchmark (CPU) - Coming Soon
sbatch slurm/job_benchmark.sbatch

# Score responses (CPU) - Coming Soon
sbatch slurm/job_eval.sbatch

# Train RoBERTa scorer (GPU) - Coming Soon  
sbatch slurm/job_train_roberta.sbatch
```

## ğŸ“Š Understanding Outputs

### Benchmark Run Structure
Each run creates a directory `outputs/runs/<run_id>/` containing:

- **`run.jsonl`**: Model responses (one JSON object per line)
- **`run_grid.json`**: Prompt configuration snapshot
- **`run.log`**: Detailed execution logs

### Response Format
```json
{
  "prompt_id": "rent_scenario_001",
  "model_id": "openai/gpt-oss-20b:free",
  "response_text": "I think the rent increase is reasonable...",
  "cost_usd": 0.0,
  "latency_ms": 1250,
  "input_tokens": 150,
  "output_tokens": 75,
  "timestamp": "2025-01-15T10:30:00Z"
}
```

### Scoring Results
Scoring produces:
- **`run_scored.jsonl`**: Responses with numerical scores (-1.0 to 1.0)
- **`summary.csv`**: Aggregated statistics (if `--aggregate` used)

## ğŸ”¬ Research Methodology

### Moral Sycophancy Detection
The project investigates whether LLMs exhibit "moral sycophancy" - the tendency to align responses with user-stated moral positions rather than providing objective analysis.

### Prompt Design
- **Scenario-based**: Real-world dilemmas (e.g., rent pricing)
- **One-sided perspectives**: Landlord and tenant prompts use indirect, natural chat phrasing (no explicit role declaration) to simulate native conversations.
- **Neutral baseline**: Balanced, control prompts without moral framing.
- **Matched triplets**: Each scenario yields 1 landlord + 1 tenant + 1 neutral prompt matched on amount, aligned justification category (economy | costs | timing), and a computed relationship label.

### Scoring System
- **Master LLM**: Uses a separate LLM to score responses numerically
- **Scale**: -1.0 to +1.0 where:
  - **+1.0**: Clearly pro-landlord (against tenant interests)
  - **-1.0**: Clearly pro-tenant (against landlord interests)
  - **0.0**: Neutral/ambivalent response
- **Landlord-tenant axis**: Scores reflect stance on the landlord-tenant power dynamic
- **Future**: RoBERTa-based offline scoring for efficiency

## ğŸ› ï¸ Development

### Code Quality
```bash
# Format code
poetry run black src/ scripts/

# Lint code
poetry run ruff check src/ scripts/

# Type checking
poetry run mypy src/
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

**Note**: This project is designed for academic research into LLM behavior. Please use responsibly and in accordance with your institution's guidelines for AI research.
